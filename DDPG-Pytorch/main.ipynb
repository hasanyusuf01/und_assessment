{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-T5PxjAffrY",
        "outputId": "41e07d42-e222-4127-eb23-e647dd42d896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.11/dist-packages (2.3.8)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "# !apt-get install -y swig\n",
        "!pip install gym box2d-py pygame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "lh2_7_33icVA",
        "outputId": "fab9cf78-05b1-40a3-e142-bfc11eaf794a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.0\n",
            "    Uninstalling numpy-2.3.0:\n",
            "      Successfully uninstalled numpy-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "45fd22981d0e426e9a788229dc0830bd",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade \"numpy<2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYmMTHQXVGE_"
      },
      "outputs": [],
      "source": [
        "# point_particle_env.py\n",
        "\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "class PointParticleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Continuous 2D navigation with interior obstacles and boundary walls.\n",
        "    - State: [x, y, heading] (plus flattened obstacles if obs=True)\n",
        "    - Action: heading angle in degrees [action_min, action_max]\n",
        "    - Reward:\n",
        "        * +goal_reward on reaching the goal\n",
        "        * -obstacle_penalty if bumping an obstacle\n",
        "        * -wall_penalty     if pushing against a boundary\n",
        "        * otherwise = -distance_to_goal\n",
        "    - Episode ends (terminated) on goal; truncated on time-limit.\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
        "\n",
        "    def __init__(self,\n",
        "                 size=(50, 50),\n",
        "                 goal=None,\n",
        "                 exploring_starts: bool=False,\n",
        "                 obs: bool=False,\n",
        "                 action_range=(0, 360),\n",
        "                 max_episode_steps: int = 500,\n",
        "                 obstacle_penalty: float = 10.0,\n",
        "                 wall_penalty: float = 10.0,\n",
        "                 goal_reward: float = 100.0):\n",
        "        super().__init__()\n",
        "        # —— seeding for reproducibility\n",
        "        self.seed()\n",
        "\n",
        "        # —— env parameters\n",
        "        self.size = np.array(size, dtype=np.float32)\n",
        "        self.speed = 5.0\n",
        "        self.goal = np.array([45.0, 45.0], dtype=np.float32) \\\n",
        "                    if goal is None else np.array(goal, dtype=np.float32)\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.obs = obs\n",
        "        self.action_min, self.action_max = action_range\n",
        "\n",
        "        # —— penalties & rewards\n",
        "        self.obstacle_penalty = obstacle_penalty\n",
        "        self.wall_penalty     = wall_penalty\n",
        "        self.goal_reward      = goal_reward\n",
        "\n",
        "        # —— time‐limit bookkeeping\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        self.current_step      = 0\n",
        "\n",
        "        # —— action & observation spaces\n",
        "        self.action_space = spaces.Box(\n",
        "            low  = np.array([self.action_min], dtype=np.float32),\n",
        "            high = np.array([self.action_max], dtype=np.float32),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # define some interior rectangular obstacles (x1,y1,x2,y2)\n",
        "        self.obstacles = [\n",
        "            (12, 12, 25, 15),\n",
        "            (25, 25, 27, 37)\n",
        "        ]\n",
        "\n",
        "        # observation space: [x, y, heading] + obstacles if obs=True\n",
        "        if self.obs:\n",
        "            obs_low  = np.zeros(4 * len(self.obstacles), dtype=np.float32)\n",
        "            obs_high = np.concatenate((self.size, [360])).astype(np.float32)\n",
        "            self.observation_space = spaces.Box(\n",
        "                low  = np.concatenate(([0, 0, 0], obs_low)),\n",
        "                high = np.concatenate((obs_high, obs_low + self.size[0])),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "        else:\n",
        "            self.observation_space = spaces.Box(\n",
        "                low  = np.array([0, 0, 0], dtype=np.float32),\n",
        "                high = np.array([*self.size, 360], dtype=np.float32),\n",
        "                dtype=np.float32\n",
        "            )\n",
        "\n",
        "        # internal\n",
        "        self.state  = None\n",
        "        self.screen = None\n",
        "\n",
        "    def seed(self, seed: Optional[int]=None):\n",
        "        \"\"\"Gym-style seeding.\"\"\"\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def reset(self, *, seed: Optional[int]=None, options=None):\n",
        "        \"\"\"Reset environment; return (obs, info).\"\"\"\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "        self.current_step = 0\n",
        "\n",
        "        if self.exploring_starts:\n",
        "            x       = self.np_random.uniform(0, self.size[0])\n",
        "            y       = self.np_random.uniform(0, self.size[1])\n",
        "            heading = self.np_random.uniform(self.action_min, self.action_max)\n",
        "        else:\n",
        "            x, y, heading = 10.0, 10.0, 0.0\n",
        "\n",
        "        if self.obs:\n",
        "            flat_obs   = np.array(self.obstacles).flatten().astype(np.float32)\n",
        "            self.state  = np.concatenate(([x, y, heading], flat_obs))\n",
        "        else:\n",
        "            self.state  = np.array([x, y, heading], dtype=np.float32)\n",
        "\n",
        "        return self.state, {}\n",
        "\n",
        "    def is_collision(self, x: float, y: float) -> bool:\n",
        "        \"\"\"Check interior obstacles only.\"\"\"\n",
        "        for x1, y1, x2, y2 in self.obstacles:\n",
        "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Apply action, return (next_state, reward, terminated, truncated, info).\"\"\"\n",
        "        self.current_step += 1\n",
        "\n",
        "        new_state, reward, terminated, info = self._simulate_step(self.state, action)\n",
        "\n",
        "        x, y = new_state[:2]\n",
        "        # interior obstacle penalty\n",
        "        if self.is_collision(x, y):\n",
        "            reward -= self.obstacle_penalty\n",
        "        # boundary wall penalty\n",
        "        if not (0 <= x <= self.size[0] and 0 <= y <= self.size[1]):\n",
        "            # (shouldn’t happen if we clip, but just in case)\n",
        "            reward -= self.wall_penalty\n",
        "\n",
        "        # goal bonus\n",
        "        if terminated:\n",
        "            reward += self.goal_reward\n",
        "\n",
        "        # truncation on time-limit\n",
        "        truncated = (self.current_step >= self.max_episode_steps)\n",
        "\n",
        "        # update state\n",
        "        self.state = new_state\n",
        "\n",
        "        return new_state, reward, terminated, truncated, info\n",
        "\n",
        "    def _simulate_step(self, state, action):\n",
        "        \"\"\"Compute next_state, base_reward, terminated (no penalties here).\"\"\"\n",
        "        pos     = state[:2]\n",
        "        heading = float(action[0])\n",
        "\n",
        "        dx = self.speed * np.cos(np.radians(heading))\n",
        "        dy = self.speed * np.sin(np.radians(heading))\n",
        "        new_x = pos[0] + dx\n",
        "        new_y = pos[1] + dy\n",
        "\n",
        "        # detect interior obstacle or boundary\n",
        "        hit_obstacle = self.is_collision(new_x, new_y)\n",
        "        hit_wall     = not (0 <= new_x <= self.size[0] and 0 <= new_y <= self.size[1])\n",
        "\n",
        "        # roll-back if collision or out-of-bounds\n",
        "        if hit_obstacle or hit_wall:\n",
        "            new_x, new_y = pos\n",
        "\n",
        "        # clip to exact bounds\n",
        "        new_x = np.clip(new_x, 0, self.size[0])\n",
        "        new_y = np.clip(new_y, 0, self.size[1])\n",
        "\n",
        "        if self.obs:\n",
        "            obs_data  = state[3:]\n",
        "            new_state = np.concatenate(([new_x, new_y, heading], obs_data))\n",
        "        else:\n",
        "            new_state = np.array([new_x, new_y, heading], dtype=np.float32)\n",
        "\n",
        "        # base reward = negative distance to goal\n",
        "        distance   = np.linalg.norm(new_state[:2] - self.goal)\n",
        "        base_reward = -distance\n",
        "\n",
        "        # termination when close to goal\n",
        "        terminated = (distance < 5.0)\n",
        "\n",
        "        return new_state, base_reward, terminated, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Draw agent, goal, obstacles, and walls.\"\"\"\n",
        "        screen_size = 600\n",
        "        scale = screen_size / max(self.size)\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))  # background\n",
        "\n",
        "        # draw boundary walls (as gray frame)\n",
        "        wall_thick = 5\n",
        "        pygame.draw.rect(surf, (100,100,100), (0, 0, screen_size, wall_thick))  # top\n",
        "        pygame.draw.rect(surf, (100,100,100), (0, 0, wall_thick, screen_size))  # left\n",
        "        pygame.draw.rect(surf, (100,100,100), (0, screen_size-wall_thick, screen_size, wall_thick))  # bottom\n",
        "        pygame.draw.rect(surf, (100,100,100), (screen_size-wall_thick, 0, wall_thick, screen_size))  # right\n",
        "\n",
        "        # draw goal\n",
        "        gx = int(self.goal[0] * scale)\n",
        "        gy = screen_size - int(self.goal[1] * scale)\n",
        "        gfxdraw.filled_circle(surf, gx, gy, int(scale*2), (40,199,172))\n",
        "\n",
        "        # draw obstacles\n",
        "        for x1, y1, x2, y2 in self.obstacles:\n",
        "            rx = int(x1*scale)\n",
        "            ry = screen_size - int(y2*scale)\n",
        "            w  = int((x2-x1)*scale)\n",
        "            h  = int((y2-y1)*scale)\n",
        "            pygame.draw.rect(surf, (128,128,128), (rx, ry, w, h))\n",
        "\n",
        "        # draw agent\n",
        "        ax = int(self.state[0]*scale)\n",
        "        ay = screen_size - int(self.state[1]*scale)\n",
        "        gfxdraw.filled_circle(surf, ax, ay, int(scale*1), (228,63,90))\n",
        "        # heading line\n",
        "        end_x = ax + int(10*np.cos(np.radians(self.state[2])))\n",
        "        end_y = ay - int(10*np.sin(np.radians(self.state[2])))\n",
        "        pygame.draw.line(surf, (255,255,255), (ax,ay), (end_x,end_y), 2)\n",
        "\n",
        "        canvas = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(canvas, (0,0))\n",
        "\n",
        "        if mode=='human':\n",
        "            pygame.display.flip()\n",
        "            return None\n",
        "        elif mode=='rgb_array':\n",
        "            arr = pygame.surfarray.pixels3d(self.screen)\n",
        "            return np.transpose(arr, (1,0,2)).astype(np.uint8)\n",
        "        else:\n",
        "            raise ValueError(f\"Render mode {mode} not supported\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Shutdown pygame.\"\"\"\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YCb48-rXVUIS",
        "outputId": "b68380e7-ec5d-488d-eb94-c2f9c821468a"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Requested MovieWriter (ffmpeg) not available",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m anim = animation.FuncAnimation(fig, update, frames=frames, interval=\u001b[32m100\u001b[39m, blit=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Display animation as HTML video (for Jupyter notebooks)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m HTML(\u001b[43manim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_html5_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genv/lib/python3.13/site-packages/matplotlib/animation.py:1302\u001b[39m, in \u001b[36mAnimation.to_html5_video\u001b[39m\u001b[34m(self, embed_limit)\u001b[39m\n\u001b[32m   1299\u001b[39m path = Path(tmpdir, \u001b[33m\"\u001b[39m\u001b[33mtemp.m4v\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1300\u001b[39m \u001b[38;5;66;03m# We create a writer manually so that we can get the\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;66;03m# appropriate size for the tag\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m Writer = \u001b[43mwriters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43manimation.writer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1303\u001b[39m writer = Writer(codec=\u001b[33m'\u001b[39m\u001b[33mh264\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1304\u001b[39m                 bitrate=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33manimation.bitrate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1305\u001b[39m                 fps=\u001b[32m1000.\u001b[39m / \u001b[38;5;28mself\u001b[39m._interval)\n\u001b[32m   1306\u001b[39m \u001b[38;5;28mself\u001b[39m.save(\u001b[38;5;28mstr\u001b[39m(path), writer=writer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genv/lib/python3.13/site-packages/matplotlib/animation.py:121\u001b[39m, in \u001b[36mMovieWriterRegistry.__getitem__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_available(name):\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._registered[name]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequested MovieWriter (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) not available\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mRuntimeError\u001b[39m: Requested MovieWriter (ffmpeg) not available"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create environment with obstacles in state\n",
        "env = PointParticleEnv()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def run_random_agent_animation(env, steps=500):\n",
        "    frames = []\n",
        "    state = env.reset()\n",
        "    for _ in range(steps):\n",
        "        # Sample a random action from the continuous action space\n",
        "        action = env.action_space.sample()\n",
        "        state, reward, done,_, info = env.step(action)\n",
        "        frame = env.render(mode='rgb_array')\n",
        "        frames.append(frame)\n",
        "        if done:\n",
        "            break\n",
        "    return frames\n",
        "\n",
        "# Create environment instance (with obstacles and continuous actions)\n",
        "env = PointParticleEnv(obs=True)\n",
        "\n",
        "# Run the agent for 50 random steps and collect frames\n",
        "frames = run_random_agent_animation(env, steps=50)\n",
        "\n",
        "# Create animation using matplotlib\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.axis('off')\n",
        "im = ax.imshow(frames[0])\n",
        "\n",
        "def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "\n",
        "anim = animation.FuncAnimation(fig, update, frames=frames, interval=100, blit=True)\n",
        "\n",
        "# Display animation as HTML video (for Jupyter notebooks)\n",
        "HTML(anim.to_html5_video())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujBgisU_UWuE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import gym\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from src.utils import *\n",
        "from src.memory import *\n",
        "from src.agents import *\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = ''\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, config_file, enable_logging=True):\n",
        "        self.enable_logging = enable_logging\n",
        "        self.config = Trainer.parse_config(config_file)\n",
        "        self.env =  gym.make(self.config['env_name'])\n",
        "        self.env = apply_seed(self.env,self.config['seed'])\n",
        "        self.state_dimension = self.env.observation_space.shape[0]\n",
        "        print(\"state\",self.state_dimension)\n",
        "        self.action_dimension = self.env.action_space.shape[0]\n",
        "        self.max_action = float(self.env.action_space.high[0])\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.agent = DDPGAgent(\n",
        "            state_dim=self.state_dimension, action_dim=self.action_dimension,\n",
        "            max_action=self.max_action, device=self.device,\n",
        "            discount=self.config['discount'], tau=self.config['tau']\n",
        "        )\n",
        "        self.save_file_name = f\"DDPG_{self.config['env_name']}_{self.config['seed']}\"\n",
        "        self.memory = ReplayBuffer()\n",
        "        if self.enable_logging:\n",
        "            wandb.init(project=\"ddpg\", config=self.config)\n",
        "        try:\n",
        "            os.mkdir('./pretrained_models')\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_config(json_file):\n",
        "        with open(json_file, 'r') as f:\n",
        "            configs = json.load(f)\n",
        "        return configs\n",
        "\n",
        "    def train(self):\n",
        "        state, info = self.env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num = 0\n",
        "        evaluations = []\n",
        "        episode_rewards = []\n",
        "        for ts in tqdm(range(1, int(self.config['time_steps']) + 1)):\n",
        "            episode_timesteps += 1\n",
        "            if ts < self.config['start_time_step']:\n",
        "                action = self.env.action_space.sample()\n",
        "            else:\n",
        "                action = (\n",
        "                        self.agent.select_action(np.array(state)) + np.random.normal(\n",
        "                    0, self.max_action * self.config['expl_noise'],\n",
        "                    size=self.action_dimension\n",
        "                )\n",
        "                ).clip(\n",
        "                    -self.max_action,\n",
        "                    self.max_action\n",
        "                )\n",
        "            next_state, reward, done, trunc, info = self.env.step(action)\n",
        "            self.memory.push(\n",
        "                state, action,reward, next_state,\n",
        "                float(done) if episode_timesteps < self.env._max_episode_steps else 0)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            if ts >= self.config['start_time_step']:\n",
        "                self.agent.train(self.memory, self.config['batch_size'])\n",
        "            if done:\n",
        "                if self.enable_logging:\n",
        "                    wandb.log({'Episode Reward': episode_reward, 'Timesteps': ts})\n",
        "                episode_rewards.append(episode_reward)\n",
        "                state, info = self.env.reset()\n",
        "                done = False\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                if episode_num % 100 == 0 and episode_num > 0:\n",
        "                    evaluations.append(evaluate_policy(self.agent, self.config['env_name'], self.config['seed'],enable_logging=self.enable_logging,wandb=wandb))\n",
        "                    self.agent.save_checkpoint(f\"./pretrained_models/{self.save_file_name}\")\n",
        "                episode_num += 1\n",
        "        wandb.finish()\n",
        "        return episode_rewards, evaluations\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.agent.load_checkpoint(f\"./pretrained_models/DDPG_{self.config['env_name']}_{self.config['seed']}\")\n",
        "        evaluate_policy(self.agent, self.config['env_name'], self.config['seed'],render=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "TgpYB1mYVzyT",
        "outputId": "c136b464-88c9-4754-df35-406211e86e55"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.box.Box'>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m gym.register(\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mPointParticle-v0\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     entry_point=PointParticleEnv,   \u001b[38;5;66;03m# or \"__main__:PointParticleEnv\"\u001b[39;00m\n\u001b[32m      8\u001b[39m     max_episode_steps=\u001b[32m500\u001b[39m    \u001b[38;5;66;03m# ← choose a sensible episode length\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     12\u001b[39m env_name=\u001b[33m'\u001b[39m\u001b[33mPointParticle-v0\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./configs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menv_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43menable_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m episode_rewards, evaluations = trainer.train()\n\u001b[32m     16\u001b[39m plt.figure(figsize=(\u001b[32m16\u001b[39m, \u001b[32m10\u001b[39m))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, config_file, enable_logging)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.enable_logging = enable_logging\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.config = Trainer.parse_config(config_file)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28mself\u001b[39m.env =  \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menv_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.env = apply_seed(\u001b[38;5;28mself\u001b[39m.env,\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.state_dimension = \u001b[38;5;28mself\u001b[39m.env.observation_space.shape[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genv/lib/python3.13/site-packages/gymnasium/envs/registration.py:669\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# Run the environment checker as the lowest level wrapper\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    667\u001b[39m     disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m spec_.disable_env_checker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    668\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m     env = \u001b[43mPassiveEnvChecker\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# Add the order enforcing wrapper\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec_.order_enforce:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genv/lib/python3.13/site-packages/gymnasium/wrappers/env_checker.py:23\u001b[39m, in \u001b[36mPassiveEnvChecker.__init__\u001b[39m\u001b[34m(self, env)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(env)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m     21\u001b[39m     env, \u001b[33m\"\u001b[39m\u001b[33maction_space\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mThe environment must specify an action space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mcheck_action_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m     25\u001b[39m     env, \u001b[33m\"\u001b[39m\u001b[33mobservation_space\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mThe environment must specify an observation space. https://gymnasium.farama.org/content/environment_creation/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m check_observation_space(env.observation_space)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/genv/lib/python3.13/site-packages/gymnasium/utils/passive_env_checker.py:74\u001b[39m, in \u001b[36mcheck_space\u001b[39m\u001b[34m(space, space_type, check_box_space_fn)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A passive check of the environment action space that should not affect the environment.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces.Space):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspace_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m space does not inherit from `gymnasium.spaces.Space`, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(space)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(space, spaces.Box):\n\u001b[32m     79\u001b[39m     check_box_space_fn(space)\n",
            "\u001b[31mAssertionError\u001b[39m: action space does not inherit from `gymnasium.spaces.Space`, actual type: <class 'gym.spaces.box.Box'>"
          ]
        }
      ],
      "source": [
        "# from train import Trainer\n",
        "from matplotlib import pyplot as plt\n",
        "import gym\n",
        "from gym.envs.registration import register\n",
        "gym.register(\n",
        "    id='PointParticle-v0',\n",
        "    entry_point=PointParticleEnv,   # or \"__main__:PointParticleEnv\"\n",
        "    max_episode_steps=500    # ← choose a sensible episode length\n",
        ")\n",
        "\n",
        "\n",
        "env_name='PointParticle-v0'\n",
        "trainer = Trainer(config_file=f'./configs/{env_name}.json',enable_logging=True)\n",
        "episode_rewards, evaluations = trainer.train()\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "plt.plot(episode_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl8f57wRbe0-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "genv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
